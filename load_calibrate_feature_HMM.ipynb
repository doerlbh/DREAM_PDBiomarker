{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data from Synapse ADI, perform a quaternion rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baihan Lin\n",
    "# Columbia University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'inlinec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9aa217c47bab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inlinec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/DoerLBH/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DoerLBH/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-104>\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n",
      "\u001b[0;32m/Users/DoerLBH/anaconda/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DoerLBH/anaconda/lib/python3.6/site-packages/IPython/core/magics/pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DoerLBH/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   2935\u001b[0m         \"\"\"\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylabtools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m         \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DoerLBH/anaconda/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mfind_gui_and_backend\u001b[0;34m(gui, gui_select)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# select backend based on requested gui\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# We need to read the backend from the original data structure, *not*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'inlinec'"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inlinec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synapseclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as pl\n",
    "import json\n",
    "# import peakutils\n",
    "# from peakutils.plot import plot as pplot\n",
    "from scipy import signal, ndimage, io, stats\n",
    "import scipy.integrate as integrate\n",
    "from math import factorial\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, Baihan Lin!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Login into the Synapse Client\n",
    "\n",
    "# syn = synapseclient.Synapse()\n",
    "syn = synapseclient.login()\n",
    "\n",
    "# Enter login and password\n",
    "#syn.login('login','password')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query demographics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographicstable = syn.tableQuery('select * from syn5511429')\n",
    "\n",
    "INPUT_DEMO_SYNID = \"syn10146552\"\n",
    "\n",
    "demo_syntable = syn.tableQuery(\"SELECT * FROM syn10146552\")\n",
    "demo = demo_syntable.asDataFrame()\n",
    "healthCodeList = \", \".join( repr(i) for i in demo[\"healthCode\"]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query multiple datasets, starting from DeviceMotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 files, 500 cached locally\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DoerLBH/anaconda/lib/python3.6/site-packages/synapseclient/client.py:3226: UserWarning: Weird file handle: None\n",
      "  warnings.warn(\"Weird file handle: %s\" % file_handle_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 files, 498 cached locally\n"
     ]
    }
   ],
   "source": [
    "INPUT_WALKING_ACTIVITY_TABLE_SYNID = \"syn10146553\"\n",
    "walkingtable = syn.tableQuery(('select * FROM {0} WHERE healthCode IN ({1}) AND \"deviceMotion_walking_outbound.json.items\" is not null LIMIT 500').format(INPUT_WALKING_ACTIVITY_TABLE_SYNID, healthCodeList))                              \n",
    "walking_df = walkingtable.asDataFrame()\n",
    "walking_df['idx'] = walking_df.index\n",
    "walking_df = walking_df.drop(['createdOn', 'appVersion', 'phoneInfo','accel_walking_outbound.json.items',\n",
    "                              'pedometer_walking_outbound.json.items','accel_walking_return.json.items',\n",
    "                              'deviceMotion_walking_return.json.items','pedometer_walking_return.json.items',\n",
    "                              'accel_walking_rest.json.items'],axis=1)\n",
    "\n",
    "walking_df = walking_df.dropna()\n",
    "\n",
    "filePaths_DMoutbound = syn.downloadTableColumns(walkingtable, ['deviceMotion_walking_outbound.json.items'])\n",
    "filePaths_DMrest = syn.downloadTableColumns(walkingtable, ['deviceMotion_walking_rest.json.items'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store DeviceMotion datasets in dataframes and pickle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'demographicstable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1d738d1514ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemographics_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdemographicstable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalking_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDMoutboundPathlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilePaths_DMoutbound\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalking_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'deviceMotion_walking_outbound.json.items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'demographicstable' is not defined"
     ]
    }
   ],
   "source": [
    "demographics_df=demographicstable.asDataFrame()\n",
    "\n",
    "wlist = np.arange(len(walking_df))\n",
    "\n",
    "DMoutboundPathlist=[filePaths_DMoutbound[str(int(walking_df.ix[entry,'deviceMotion_walking_outbound.json.items']))] for entry in wlist] \n",
    "DMrestPathlist = [filePaths_DMrest[str(int(walking_df.ix[entry,'deviceMotion_walking_rest.json.items']))] for entry in wlist]\n",
    "\n",
    "walking_df['DMoutboundPaths'] = DMoutboundPathlist\n",
    "walking_df['DMrestPaths'] = DMrestPathlist\n",
    "\n",
    "demographics_df.to_pickle('demographics_df.pkl')\n",
    "walking_df.to_pickle('walking_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform a quaternion rotation on the accelerometer files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function makes sure that the each quaternion in the angular phone information vector is properly normalized. The\n",
    "# magnitude should be close to 1 within some tolerance. If it is not, rescale the quaternion.\n",
    "\n",
    "def normalize(v, tolerance=0.00001):\n",
    "    mag2 = sum(n * n for n in v)\n",
    "    if abs(mag2 - 1.0) > tolerance:\n",
    "        mag = sqrt(mag2)\n",
    "        v = tuple(n / mag for n in v)\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This funciton returns the conjugate of the quaternion\n",
    "\n",
    "def get_quaternion_conjugate(quaternion):\n",
    "    w, x, y, z = quaternion\n",
    "    return (w, -x, -y, -z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function multiples two quaternions\n",
    "\n",
    "def multiply_quaternions(quaternion1, quaternion2):\n",
    "    w1, x1, y1, z1 = quaternion1\n",
    "    w2, x2, y2, z2 = quaternion2\n",
    "    # rearranged a little for my own clarity\n",
    "    w = (w1 * w2) - (x1 * x2) - (y1 * y2) - (z1 * z2)\n",
    "    x = (w1 * x2) + (x1 * w2) + (y1 * z2) - (z1 * y2)\n",
    "    y = (w1 * y2) - (x1 * z2) + (y1 * w2) + (z1 * x2)\n",
    "    z = (w1 * z2) + (x1 * y2) - (y1 * x2) + (z1 * w2)  \n",
    "    return w, x, y, z\n",
    "\n",
    "def multiply_quaternionANDvector(q1, v1):\n",
    "    q1 = normalize(q1,tolerance = 0.00001) \n",
    "    q2 = (0.0,) + v1\n",
    "    return multiply_quaternions(multiply_quaternions(q1, q2), get_quaternion_conjugate(q1))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine into a datastructure from all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfdata= {'time':[],'rotX':[],'rotY':[],'rotZ':[],'total_rawacceleration':[],'rest_time':[],'rest_rotX':[],'rest_rotY':[],'rest_rotZ':[],'rest_total_rawacceleration':[],'healthCode':[]}\n",
    "Accelerometer_df = pd.DataFrame(dfdata, columns =['time','rotX','rotY','rotZ','total_rawacceleration','rest_time','rest_rotX','rest_rotY','rest_rotZ','rest_total_rawacceleration','healthCode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load accelerometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mypkls =['0_2500','2500_5000','5000_7500','7500_10000','10000_12500','12500_15000','15000_17500',\n",
    "         '17500_20000','20000_22500','22500_23093']\n",
    "\n",
    "for entry in range(0,2500):\n",
    "    \n",
    "    if entry % 250 == 0:\n",
    "        print entry\n",
    "        \n",
    "    healthCode = walking_df['healthCode'][entry]\n",
    "    recordId = walking_df['recordId'][entry]\n",
    "    medTimepoint = walking_df['medTimepoint'][entry]\n",
    "    walkPath = walking_df['DMoutboundPaths'][entry]\n",
    "    restPath = walking_df['DMrestPaths'][entry]\n",
    "\n",
    "    walkingrecord = [json.loads(line) for line in open(walkPath)]\n",
    "    restrecord = [json.loads(line) for line in open(restPath)]\n",
    "    \n",
    "    walkingrecordlist=walkingrecord[0]\n",
    "    restrecordlist=restrecord[0]\n",
    "    \n",
    "    barlist = np.arange(len(walkingrecordlist))\n",
    "    restbarlist = np.arange(len(restrecordlist))\n",
    "    \n",
    "    time = []\n",
    "    acceleration_vector = []\n",
    "    attitude_quaternion = []\n",
    "    rest_acceleration_vector = []\n",
    "    attitude_quaternion = []\n",
    "    \n",
    "    time = [(walkingrecordlist[bar]['timestamp']) for bar in barlist]\n",
    "    acceleration_vector = [(walkingrecordlist[bar]['userAcceleration']['x'],\n",
    "                            walkingrecordlist[bar]['userAcceleration']['y'],\n",
    "                            walkingrecordlist[bar]['userAcceleration']['z']) for bar in barlist]\n",
    "    attitude_quaternion = [(walkingrecordlist[bar]['attitude']['w'],\n",
    "                            walkingrecordlist[bar]['attitude']['x'],\n",
    "                            walkingrecordlist[bar]['attitude']['y'],\n",
    "                            walkingrecordlist[bar]['attitude']['z']) for bar in barlist]\n",
    "    \n",
    "    rest_time = [(restrecordlist[restbar]['timestamp']) for restbar in restbarlist]\n",
    "    rest_acceleration_vector = [(restrecordlist[restbar]['userAcceleration']['x'],\n",
    "                            restrecordlist[restbar]['userAcceleration']['y'],\n",
    "                            restrecordlist[restbar]['userAcceleration']['z']) for bar in restbarlist]\n",
    "    rest_attitude_quaternion = [(restrecordlist[restbar]['attitude']['w'],\n",
    "                            restrecordlist[restbar]['attitude']['x'],\n",
    "                            restrecordlist[restbar]['attitude']['y'],\n",
    "                            restrecordlist[restbar]['attitude']['z']) for restbar in restbarlist]\n",
    "    \n",
    "    # Calculate quadratic mean of original acceleration signal - this will be used as another directionless \"axis\" in my feature calculations\n",
    "    [X, Y, Z] = zip(*acceleration_vector) \n",
    "    total_rawacceleration = np.sqrt(np.square(X)+np.square(Y)+np.square(Z))\n",
    "    [restX, restY, restZ] = zip(*rest_acceleration_vector) \n",
    "    rest_total_rawacceleration = np.sqrt(np.square(restX)+np.square(restY)+np.square(restZ))\n",
    "    \n",
    "    # Apply quaternion rotations here\n",
    "    plist = np.arange(len(acceleration_vector))\n",
    "    quatrot = [multiply_quaternionANDvector(attitude_quaternion[p],acceleration_vector[p]) for p in plist]\n",
    "    [rotX, rotY, rotZ] = zip(*quatrot) \n",
    "    \n",
    "    rlist = np.arange(len(rest_acceleration_vector))\n",
    "    rest_quatrot = [multiply_quaternionANDvector(rest_attitude_quaternion[r],rest_acceleration_vector[r]) for r in rlist]\n",
    "    [rest_rotX, rest_rotY, rest_rotZ] = zip(*rest_quatrot) \n",
    "    \n",
    "    dfdata= {'time':[],'rotX':[],'rotY':[],'rotZ':[],'total_rawacceleration':[],'rest_time':[],'rest_rotX':[],'rest_rotY':[],'rest_rotZ':[],'rest_total_rawacceleration':[],'healthCode':[]}\n",
    "\n",
    "    Accelerometer_df = Accelerometer_df.append({'time':time,'rotX':rotX,'rotY':rotY,'rotZ':rotZ,\n",
    "                                                'total_rawacceleration':total_rawacceleration,'rest_time':rest_time,\n",
    "                                                'rest_rotX':rest_rotX,'rest_rotY':rest_rotY,'rest_rotZ':rest_rotZ,\n",
    "                                                'rest_total_rawacceleration':rest_total_rawacceleration,\n",
    "                                                'healthCode':healthCode, 'recordId':recordId,'medTimepoint':medTimepoint},ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross-reference accelerometer data with demographics data and pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add relevant demographics data to walking activity feature data frame\n",
    "Demographics_df = demographics_df.drop(['recordId','createdOn','appVersion','phoneInfo','are-caretaker', 'deep-brain-stimulation',\n",
    "                                        'education','employment','health-history','healthcare-provider',\n",
    "                                        'home-usage','last-smoked','maritalStatus','medical-usage',\n",
    "                                        'medical-usage-yesterday','packs-per-day',\n",
    "                                        'past-participation','phone-usage','race','smartphone',\n",
    "                                        'smoked','surgery','video-usage','years-smoking',\n",
    "                                        'diagnosis-year','medication-start-year','onset-year'],axis=1)\n",
    "Combined_df = pd.merge(Accelerometer_df, Demographics_df, on='healthCode')\n",
    "\n",
    "Final_df.to_pickle('Accelerometer_Demographics_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Plot a sample time series\n",
    "\n",
    "\n",
    "\n",
    "pl.figure()\n",
    "pl.plot(fpr, tpr, 'g',label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "pl.plot([0, 1], [0, 1], 'k--')\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.05])\n",
    "pl.xticks(color = 'k', size = 28)\n",
    "pl.yticks(color = 'k', size = 28)\n",
    "pl.xlabel('False Positive Rate',{'color':'k','fontsize': 28})\n",
    "pl.ylabel('True Positive Rate',{'color':'k','fontsize': 28})\n",
    "pl.title('Receiver operating characteristic',{'color':'k','fontsize': 28})\n",
    "pl.legend(loc=\"lower right\",fontsize= 18)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampling interval and frequency\n",
    "samplingint = 0.01\n",
    "Fs = 1/samplingint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ENTER FILENAME!\n",
    "filename = 'Accelerometer_Demographics_df'\n",
    "\n",
    "thresholdingparam = 1.5\n",
    "peakfindinterval = 80\n",
    "\n",
    "minnumpeaksfound = 5\n",
    "maxnumpeaksfound = 20\n",
    "numpeakintervalstoavg = 5\n",
    "\n",
    "maxpoweratrest = 0.00005\n",
    "\n",
    "FFTsamples = 1024\n",
    "minsignallength = 1200\n",
    "\n",
    "RESTFFTsamples = 512\n",
    "RESTminsignallength = 515\n",
    "\n",
    "# buffer for epochs between steps\n",
    "buff = 5\n",
    "minchunksamples = 90\n",
    "\n",
    "# Accelerometer_df = pd.read_pickle(filename)\n",
    "recordnums = Accelerometer_df.index # important - some indices won't be there since we dropped records before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: Savitzky Golay Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Savitzky Golay filter from http://scipy.github.io/old-wiki/pages/Cookbook/SavitzkyGolay\n",
    "def savitzky_golay(y, window_size, order, deriv=0, rate=1):\n",
    "    try:\n",
    "        window_size = np.abs(np.int(window_size))\n",
    "        order = np.abs(np.int(order))\n",
    "    except ValueError, msg:\n",
    "        raise ValueError(\"window_size and order have to be of type int\")\n",
    "    if window_size % 2 != 1 or window_size < 1:\n",
    "        raise TypeError(\"window_size size must be a positive odd number\")\n",
    "    if window_size < order + 2:\n",
    "        raise TypeError(\"window_size is too small for the polynomials order\")\n",
    "    order_range = range(order+1)\n",
    "    half_window = (window_size -1) // 2\n",
    "    # precompute coefficients\n",
    "    b = np.mat([[k**i for i in order_range] for k in range(-half_window, half_window+1)])\n",
    "    m = np.linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)\n",
    "    # pad the signal at the extremes with values taken from the signal itself\n",
    "    firstvals = y[0] - np.abs( y[1:half_window+1][::-1] - y[0] )\n",
    "    lastvals = y[-1] + np.abs(y[-half_window-1:-1][::-1] - y[-1])\n",
    "    y = np.concatenate((firstvals, y, lastvals))\n",
    "    return np.convolve( m[::-1], y, mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_FFT(timeseries, samplingint):\n",
    "    fouriercoefficients = np.fft.rfft(timeseries)\n",
    "    normalization = 2.0/len(timeseries)\n",
    "    fouriercoefficients = fouriercoefficients*normalization\n",
    "    frequencies = np.fft.rfftfreq(len(timeseries),samplingint)\n",
    "    return frequencies, fouriercoefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: Find Peaks with PeakUtils threshold peak finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_peak_info(timesignal,processedsignal,sampling_int,thresholdingparam,peakfindinterval):\n",
    "    AVGsignal = np.mean(processedsignal)\n",
    "    minthreshold = thresholdingparam*AVGsignal\n",
    "    peakindices = peakutils.indexes(processedsignal ,thres=minthreshold,min_dist=peakfindinterval)\n",
    "    peaktimes = peakindices*sampling_int\n",
    "    peakamplitudes = timesignal[peakindices]\n",
    "    \n",
    "    return peakindices, peaktimes, peakamplitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize dataframe for storing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureinfo = {'healthCode':[],'record':[],'age':[],'gender':[],'professional-diagnosis':[],'recordId':[],\n",
    "               'totpowerX':[],'totpowerY':[],'totpowerZ':[],\n",
    "               'powentropyX':[], 'powentropyY':[],'powentropyZ':[],\n",
    "               'numpeaksX':[],'numpeaksY':[],'numpeaksZ':[],\n",
    "               'MEANpeakintX':[],'MEANpeakintY':[],'MEANpeakintZ':[],\n",
    "               'CVpeakintX':[],'CVpeakintY':[],'CVpeakintZ':[],'duration':[],\n",
    "               'FFT1Z':[],'FFT2Z':[],'FFT3Z':[],\n",
    "               'modeFFT1chunksX':[], 'SDFFT1chunksX':[], 'modeFFT2chunksX':[],'SDFFT2chunksX':[], 'modeFFT3chunksX':[], 'SDFFT3chunksX':[],\n",
    "               'meanpowchunksX':[],'CVpowchunksX':[],'meanentropychunksX':[],'CVentropychunksX':[],\n",
    "               'modeFFT1chunksY':[], 'SDFFT1chunksY':[],'modeFFT2chunksY':[], 'SDFFT2chunksY':[], 'modeFFT3chunksY':[], 'SDFFT3chunksY':[],\n",
    "               'meanpowchunksY':[],'CVpowchunksY':[],'meanentropychunksY':[],'CVentropychunksY':[],\n",
    "               'modeFFT1chunksZ':[], 'SDFFT1chunksZ':[], 'modeFFT2chunksZ':[], 'SDFFT2chunksZ':[], 'modeFFT3chunksZ':[], 'SDFFT3chunksZ':[],\n",
    "               'meanpowchunksZ':[],'CVpowchunksZ':[],'meanentropychunksZ':[],'CVentropychunksZ':[],\n",
    "               'onset_lag':[],'sumabs_acceleration':[]}\n",
    "\n",
    "feature_df = pd.DataFrame(featureinfo,columns =['healthCode','record','age', 'gender','professional-diagnosis','recordId',\n",
    "                                                'totpowerX','totpowerY','totpowerZ',\n",
    "                                                'powentropyX','powentropyY','powentropyZ',\n",
    "                                                'numpeaksX','numpeaksY','numpeaksZ',\n",
    "                                                'MEANpeakintX','MEANpeakintY','MEANpeakintZ',\n",
    "                                                'CVpeakintX','CVpeakintY','CVpeakintZ',\n",
    "                                                'duration','FFT1Z','FFT2Z','FFT3Z',\n",
    "                                                'modeFFT1chunksX','modeFFT2chunksX','SDFFT1chunksX', 'SDFFT2chunksX', 'modeFFT3chunksX', 'SDFFT3chunksX', \n",
    "                                                'meanpowchunksX','CVpowchunksX','meanentropychunksX','CVentropychunksX',\n",
    "                                                'modeFFT1chunksY', 'SDFFT1chunksY', 'modeFFT2chunksY', 'SDFFT2chunksY', 'modeFFT3chunksY', 'SDFFT3chunksY',\n",
    "                                                'meanpowchunksY','CVpowchunksY','meanentropychunksY','CVentropychunksY',\n",
    "                                                'modeFFT1chunksZ', 'SDFFT1chunksZ','modeFFT2chunksZ', 'SDFFT2chunksZ', 'modeFFT3chunksZ', 'SDFFT3chunksZ', \n",
    "                                                'meanpowchunksZ','CVpowchunksZ','meanentropychunksZ','CVentropychunksZ',\n",
    "                                                'onset_lag','sumabs_acceleration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: calculate features for the recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featuregrabber(record,signame,sig,datamat,datamat_YSG,datamat_processed,samplingint,thresholdingparam,peakfindinterval,minsignallength,minnumpeaksfound,buff):\n",
    "    \n",
    "    # initialize lists for storing calculations for each epoch or \"chunk\" between steps\n",
    "    FFT1chunks = []\n",
    "    FFT2chunks = []\n",
    "    FFT3chunks = []\n",
    "    powerchunks = []\n",
    "    entropychunks = []\n",
    "    \n",
    "    # calculate the duration of the recording\n",
    "    duration = (len(datamat[sig]))*samplingint\n",
    "    \n",
    "    # take recordings that are longer than the minimal length\n",
    "    if len(datamat[sig]) >= minsignallength:\n",
    "        [peakindices, peaktimes, peakamplitudes] = get_peak_info(datamat[sig],datamat_processed[sig],samplingint,thresholdingparam,peakfindinterval)\n",
    "        stepintervals = np.diff(peaktimes)\n",
    "        numpeaks = len(peakindices)\n",
    "\n",
    "        # take recordings where the number of peaks discovered is within the desired range\n",
    "        if numpeaks > minnumpeaksfound and numpeaks <= maxnumpeaksfound:\n",
    "            onset_lag = peakindices[0]*samplingint\n",
    "            for k in np.arange(numpeaks-2):\n",
    "                peakstart = peakindices[k]+buff\n",
    "                peakend = peakindices[k+1]-buff\n",
    "                chunklen = peakend-peakstart\n",
    "                midchunk = peakstart+int(chunklen/2)\n",
    "                if chunklen > minchunksamples:\n",
    "                    chunk = datamat_YSG[sig][midchunk-(minchunksamples/2):midchunk+(minchunksamples/2)]\n",
    "                    fYSG_chunk,Pxx_denYSG_chunk = signal.welch(chunk, Fs)#nperseg=128)\n",
    "                    \n",
    "                    freqchunk, ffcoeffchunk = get_FFT(chunk,samplingint)\n",
    "                    ffcoeffchunk = np.absolute(ffcoeffchunk)**2\n",
    "            \n",
    "                    ff_chunkdf = pd.DataFrame({'fouriercoefficients':ffcoeffchunk,'frequencies':freqchunk})\n",
    "                    ff_chunkdf=ff_chunkdf.sort_values('fouriercoefficients',ascending = False)\n",
    "            \n",
    "                    if ff_chunkdf.iloc[0]['frequencies'] != 0:\n",
    "                        FFT1chunks.append(ff_chunkdf.iloc[0]['frequencies'])\n",
    "                        FFT2chunks.append(ff_chunkdf.iloc[1]['frequencies'])\n",
    "                        FFT3chunks.append(ff_chunkdf.iloc[2]['frequencies'])\n",
    "                        totchunkpower = np.trapz(Pxx_denYSG_chunk)\n",
    "\n",
    "                        entropychunk = stats.entropy(Pxx_denYSG_chunk/totchunkpower)\n",
    "                        powerchunks.append(totchunkpower)\n",
    "                        entropychunks.append(entropychunk)\n",
    "                        \n",
    "            \n",
    "            # take a central part of the recording (same length for all recordings), and calculate frequency spectrum features\n",
    "            midsignal = int(len(datamat_YSG[sig])/2)\n",
    "            FFTsignal = datamat_YSG[sig][(midsignal-FFTsamples/2):(midsignal+FFTsamples/2)]\n",
    "            \n",
    "            f,Pow = signal.welch(FFTsignal, Fs,scaling='density')\n",
    "            totpower = np.trapz(Pow)\n",
    "            powentropy = stats.entropy(Pow/totpower)\n",
    "            \n",
    "   \n",
    "            frequencies, fouriercoefficients = get_FFT(FFTsignal,samplingint)\n",
    "            fouriercoefficients = np.absolute(fouriercoefficients)**2\n",
    "            ff_df = pd.DataFrame({'fouriercoefficients':fouriercoefficients,'frequencies':frequencies})\n",
    "            ff_df = ff_df.sort_values('fouriercoefficients',ascending = False)\n",
    "        \n",
    "            if ff_df.iloc[0]['frequencies'] != 0:\n",
    "                FFT1 = ff_df.iloc[0]['frequencies']\n",
    "                FFT2 = ff_df.iloc[1]['frequencies']\n",
    "                FFT3 = ff_df.iloc[2]['frequencies']\n",
    "            \n",
    "            else:\n",
    "                FFT1 = float('nan')\n",
    "                FFT2 = float('nan')\n",
    "                FFT3 = float('nan')\n",
    "        else:\n",
    "            onset_lag = float('nan')\n",
    "            totpower = float('nan')\n",
    "            powentropy = float('nan')\n",
    "            FFT1 = float('nan')\n",
    "            FFT2 = float('nan')\n",
    "            FFT3 = float('nan')\n",
    "     \n",
    "    else:\n",
    "        numpeaks = float('nan')\n",
    "        stepintervals = float('nan')\n",
    "        totpower = float('nan')\n",
    "        powentropy = float('nan')\n",
    "        FFT1 = float('nan')\n",
    "        FFT2 = float('nan')\n",
    "        FFT3 = float('nan')\n",
    "        onset_lag = float('nan')\n",
    "        \n",
    "    return duration, onset_lag, numpeaks, stepintervals, totpower, powentropy, FFT1, FFT2, FFT3, FFT1chunks, FFT2chunks, FFT3chunks, powerchunks, entropychunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: calculate features within epochs or \"chunks\" between steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunkstats(FFT1chunks, FFT2chunks, FFT3chunks, powerchunks, entropychunks):\n",
    "    if len(FFT1chunks)>=1:\n",
    "        modeFFT1chunks = mode(FFT1chunks)[0][0]\n",
    "        meanpowchunks = np.mean(powerchunks)\n",
    "        SDFFT1chunks = np.std(FFT1chunks)\n",
    "        CVpowchunks = np.std(powerchunks) / meanpowchunks\n",
    "        meanentropychunks = np.mean(entropychunks)\n",
    "        CVentropychunks = np.std(entropychunks) / meanentropychunks\n",
    "        \n",
    "    else:\n",
    "        modeFFT1chunks = float('nan')  \n",
    "        meanpowchunks = float('nan')\n",
    "        SDFFT1chunks = float('nan')\n",
    "        CVpowchunks = float('nan')\n",
    "        meanentropychunks = float('nan')\n",
    "        CVentropychunks = float('nan')\n",
    "        \n",
    "    if len(FFT2chunks)>=1:\n",
    "        modeFFT2chunks = mode(FFT2chunks)[0][0]\n",
    "        SDFFT2chunks = np.std(FFT2chunks)\n",
    "        \n",
    "    else:\n",
    "        modeFFT2chunks = float('nan')  \n",
    "        SDFFT2chunks = float('nan')\n",
    "        \n",
    "            \n",
    "    if len(FFT3chunks)>=1:\n",
    "        modeFFT3chunks = mode(FFT3chunks)[0][0]\n",
    "        SDFFT3chunks = np.std(FFT3chunks)\n",
    "        \n",
    "    else:\n",
    "        modeFFT3chunks = float('nan')  \n",
    "        SDFFT3chunks = float('nan')\n",
    "        \n",
    "    return modeFFT1chunks, SDFFT1chunks, modeFFT2chunks, SDFFT2chunks, modeFFT3chunks, SDFFT3chunks, meanpowchunks,CVpowchunks, meanentropychunks,CVentropychunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Go through each record in the data frame, calculate features and store in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for record in recordnums:\n",
    "    \n",
    "    ##################################### WALKING DATA ###################################\n",
    "    \n",
    "    time = Accelerometer_df['time'][record]\n",
    "    datamat = np.array([np.asarray(Accelerometer_df['rotX'][record]), np.asarray(Accelerometer_df['rotY'][record]),\n",
    "                        np.asarray(Accelerometer_df['rotZ'][record]), np.asarray(Accelerometer_df['total_rawacceleration'][record])])\n",
    "\n",
    "    axisitems  = np.arange(0,len(datamat))\n",
    "    \n",
    "    # Apply Savitsky Golay filter\n",
    "    datamat_YSG = [savitzky_golay(datamat[myaxis], window_size =51, order=4) for myaxis in axisitems]\n",
    "    \n",
    "    # Apply Savitsky Golay filter to squared signal for peak finding\n",
    "    datamat_YSG_peaks = [savitzky_golay(np.square(datamat[myaxis]), window_size=51, order=4) for myaxis in axisitems]\n",
    "    datamat_processed = datamat_YSG_peaks\n",
    "     \n",
    "    \n",
    "    # X\n",
    "    durationX, onset_lagX, numpeaksX, stepintervalsX, totpowerX, powentropyX, FFT1X, FFT2X, FFT3X, FFT1chunksX, FFT2chunksX, FFT3chunksX, powerchunksX, entropychunksX = featuregrabber(str(record),'X',0,datamat,datamat_YSG,datamat_processed,samplingint,thresholdingparam,peakfindinterval,minsignallength,minnumpeaksfound,buff)\n",
    "    modeFFT1chunksX, SDFFT1chunksX, modeFFT2chunksX, SDFFT2chunksX, modeFFT3chunksX, SDFFT3chunksX, meanpowchunksX,CVpowchunksX,meanentropychunksX,CVentropychunksX  = chunkstats(FFT1chunksX, FFT2chunksX, FFT3chunksX, powerchunksX, entropychunksX)\n",
    "    \n",
    "    \n",
    "    # Y\n",
    "    durationY, onset_lagY, numpeaksY, stepintervalsY, totpowerY, powentropyY, FFT1Y, FFT2Y, FFT3Y, FFT1chunksY, FFT2chunksY, FFT3chunksY, powerchunksY, entropychunksY = featuregrabber(str(record),'Y',1,datamat,datamat_YSG,datamat_processed,samplingint,thresholdingparam,peakfindinterval,minsignallength,minnumpeaksfound,buff)\n",
    "    modeFFT1chunksY, SDFFT1chunksY, modeFFT2chunksY, SDFFT2chunksY, modeFFT3chunksY, SDFFT3chunksY, meanpowchunksY,CVpowchunksY,meanentropychunksY,CVentropychunksY  = chunkstats(FFT1chunksY, FFT2chunksY, FFT3chunksY, powerchunksY, entropychunksY)\n",
    "    \n",
    "\n",
    "    # Z\n",
    "    durationZ, onset_lagZ, numpeaksZ, stepintervalsZ, totpowerZ, powentropyZ, FFT1Z, FFT2Z, FFT3Z, FFT1chunksZ, FFT2chunksZ, FFT3chunksZ, powerchunksZ, entropychunksZ = featuregrabber(str(record),'Z',2,datamat,datamat_YSG,datamat_processed,samplingint,thresholdingparam,peakfindinterval,minsignallength,minnumpeaksfound,buff)\n",
    "    modeFFT1chunksZ, SDFFT1chunksZ, modeFFT2chunksZ, SDFFT2chunksZ, modeFFT3chunksZ, SDFFT3chunksZ, meanpowchunksZ,CVpowchunksZ,meanentropychunksZ,CVentropychunksZ  = chunkstats(FFT1chunksZ, FFT2chunksZ, FFT3chunksZ, powerchunksZ, entropychunksZ)\n",
    "    \n",
    "    \n",
    "    # QUADRATIC SUM OF X,Y,Z\n",
    "    durationSUM, onset_lagSUM,numpeaksSUM, stepintervalsSUM, totpowerSUM, powentropySUM, FFT1SUM, FFT2SUM, FFT3SUM, FFT1chunksSUM, FFT2chunksSUM, FFT3chunksSUM, powerchunksSUM, entropychunksSUM = featuregrabber(str(record),'SUM',3,datamat,datamat_YSG,datamat_processed,samplingint,thresholdingparam,peakfindinterval,minsignallength,minnumpeaksfound,buff)\n",
    "    modeFFT1chunksSUM, SDFFT1chunksSUM, modeFFT2chunksSUM, SDFFT2chunksSUM, modeFFT3chunksSUM, SDFFT3chunksSUM, meanpowchunksSUM,CVpowchunksSUM,meanentropychunksSUM,CVentropychunksSUM = chunkstats(FFT1chunksSUM, FFT2chunksSUM, FFT3chunksSUM, powerchunksSUM, entropychunksSUM)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get metadata for the record\n",
    "    healthCode = Accelerometer_df['healthCode'][record]\n",
    "    age = Accelerometer_df['age'][record]\n",
    "    gender = Accelerometer_df['gender'][record]\n",
    "    professional_diagnosis = Accelerometer_df['professional-diagnosis'][record]\n",
    "    recordId = Accelerometer_df['recordId'][record]\n",
    "    \n",
    "    # Make final feature calculations\n",
    "    MEANpeakintX = np.mean(stepintervalsX)\n",
    "    MEANpeakintY = np.mean(stepintervalsY)\n",
    "    MEANpeakintZ = np.mean(stepintervalsZ)\n",
    "    CVpeakintX = np.std(stepintervalsX) / MEANpeakintX\n",
    "    CVpeakintY = np.std(stepintervalsY) / MEANpeakintY\n",
    "    CVpeakintZ = np.std(stepintervalsZ) / MEANpeakintZ\n",
    "    duration = durationZ\n",
    "    onset_lag = onset_lagZ\n",
    "    sumabs_acceleration = sum(abs(datamat[3]))\n",
    "\n",
    "   \n",
    "    feature_df = feature_df.append({'healthCode':healthCode,'record':record,'age':age, 'gender':gender, 'professional-diagnosis':professional_diagnosis,'recordId':recordId, \n",
    "                                    'totpowerX':totpowerX,'totpowerY':totpowerY,'totpowerZ':totpowerZ,\n",
    "                                    'powentropyX':powentropyX, 'powentropyY':powentropyY,'powentropyZ':powentropyZ,\n",
    "                                    'numpeaksX':numpeaksX,'numpeaksY':numpeaksY,'numpeaksZ':numpeaksZ,\n",
    "                                    'MEANpeakintX':MEANpeakintX,'MEANpeakintY':MEANpeakintY,'MEANpeakintZ':MEANpeakintZ,\n",
    "                                    'CVpeakintX':CVpeakintX,'CVpeakintY':CVpeakintY,'CVpeakintZ':CVpeakintZ,\n",
    "                                    'duration':duration,'FFT1Z':FFT1Z,'FFT2Z':FFT2Z,'FFT3Z':FFT3Z,  \n",
    "                                    'modeFFT1chunksX':modeFFT1chunksX,'SDFFT1chunksX':SDFFT1chunksX,\n",
    "                                    'modeFFT2chunksX':modeFFT2chunksX,'SDFFT2chunksX':SDFFT2chunksX,\n",
    "                                    'modeFFT3chunksX':modeFFT3chunksX,'SDFFT3chunksX':SDFFT3chunksX,\n",
    "                                    'meanpowchunksX':meanpowchunksX,'CVpowchunksX':CVpowchunksX,\n",
    "                                    'meanentropychunksX':meanentropychunksX,'CVentropychunksX':CVentropychunksX,\n",
    "                                    'modeFFT1chunksY':modeFFT1chunksY,'SDFFT1chunksY':SDFFT1chunksY,\n",
    "                                    'modeFFT2chunksY':modeFFT2chunksY,'SDFFT2chunksY':SDFFT2chunksY,\n",
    "                                    'modeFFT3chunksY':modeFFT3chunksY,'SDFFT3chunksY':SDFFT3chunksY,\n",
    "                                    'meanpowchunksY':meanpowchunksY,'CVpowchunksY':CVpowchunksY,\n",
    "                                    'meanentropychunksY':meanentropychunksY,'CVentropychunksY':CVentropychunksY,\n",
    "                                    'modeFFT1chunksZ':modeFFT1chunksZ,'SDFFT1chunksZ':SDFFT1chunksZ,\n",
    "                                    'modeFFT2chunksZ':modeFFT2chunksZ,'SDFFT2chunksZ':SDFFT2chunksZ,\n",
    "                                    'modeFFT3chunksZ':modeFFT3chunksZ,'SDFFT3chunksZ':SDFFT3chunksZ,\n",
    "                                    'meanpowchunksZ':meanpowchunksZ,'CVpowchunksZ':CVpowchunksZ,\n",
    "                                    'meanentropychunksZ':meanentropychunksZ,'CVentropychunksZ':CVentropychunksZ,\n",
    "                                    'onset_lag':onset_lag,'sumabs_acceleration':sumabs_acceleration},ignore_index=True)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Remove NANs and pickle the resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanfeature_df = feature_dfmod.dropna()\n",
    "cleanfeature_df.to_pickle('feature_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
