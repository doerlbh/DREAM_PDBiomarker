{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data from Synapse ADI, perform a quaternion rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reset\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import synapseclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as pl\n",
    "import json\n",
    "import peakutils\n",
    "from peakutils.plot import plot as pplot\n",
    "from scipy import signal, ndimage, io, stats\n",
    "import scipy.integrate as integrate\n",
    "from math import factorial\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Login into the Synapse Client\n",
    "\n",
    "# syn = synapseclient.Synapse()\n",
    "syn = synapseclient.login()\n",
    "\n",
    "# Enter login and password\n",
    "#syn.login('login','password')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query demographics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# demographicstable = syn.tableQuery('select * from syn5511429')\n",
    "\n",
    "INPUT_DEMO_SYNID = \"syn10146552\"\n",
    "\n",
    "demo_syntable = syn.tableQuery(\"SELECT * FROM syn10146552\")\n",
    "demo = demo_syntable.asDataFrame()\n",
    "healthCodeList = \", \".join( repr(i) for i in demo[\"healthCode\"]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query multiple datasets, starting from DeviceMotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT_WALKING_ACTIVITY_TABLE_SYNID = \"syn10146553\"\n",
    "walkingtable = syn.tableQuery(('select * FROM {0} WHERE healthCode IN ({1}) AND \"deviceMotion_walking_outbound.json.items\" is not null LIMIT 500').format(INPUT_WALKING_ACTIVITY_TABLE_SYNID, healthCodeList))                              \n",
    "walking_df = walkingtable.asDataFrame()\n",
    "walking_df['idx'] = walking_df.index\n",
    "walking_df = walking_df.drop(['createdOn', 'appVersion', 'phoneInfo','accel_walking_outbound.json.items',\n",
    "                              'pedometer_walking_outbound.json.items','accel_walking_return.json.items',\n",
    "                              'deviceMotion_walking_return.json.items','pedometer_walking_return.json.items',\n",
    "                              'accel_walking_rest.json.items'],axis=1)\n",
    "\n",
    "walking_df = walking_df.dropna()\n",
    "\n",
    "filePaths_DMoutbound = syn.downloadTableColumns(walkingtable, ['deviceMotion_walking_outbound.json.items'])\n",
    "filePaths_DMrest = syn.downloadTableColumns(walkingtable, ['deviceMotion_walking_rest.json.items'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store DeviceMotion datasets in dataframes and pickle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "demographics_df=demographicstable.asDataFrame()\n",
    "\n",
    "wlist = np.arange(len(walking_df))\n",
    "\n",
    "DMoutboundPathlist=[filePaths_DMoutbound[str(int(walking_df.ix[entry,'deviceMotion_walking_outbound.json.items']))] for entry in wlist] \n",
    "DMrestPathlist = [filePaths_DMrest[str(int(walking_df.ix[entry,'deviceMotion_walking_rest.json.items']))] for entry in wlist]\n",
    "\n",
    "walking_df['DMoutboundPaths'] = DMoutboundPathlist\n",
    "walking_df['DMrestPaths'] = DMrestPathlist\n",
    "\n",
    "demographics_df.to_pickle('demographics_df.pkl')\n",
    "walking_df.to_pickle('walking_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform a quaternion rotation on the accelerometer files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function makes sure that the each quaternion in the angular phone information vector is properly normalized. The\n",
    "# magnitude should be close to 1 within some tolerance. If it is not, rescale the quaternion.\n",
    "\n",
    "def normalize(v, tolerance=0.00001):\n",
    "    mag2 = sum(n * n for n in v)\n",
    "    if abs(mag2 - 1.0) > tolerance:\n",
    "        mag = sqrt(mag2)\n",
    "        v = tuple(n / mag for n in v)\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This funciton returns the conjugate of the quaternion\n",
    "\n",
    "def get_quaternion_conjugate(quaternion):\n",
    "    w, x, y, z = quaternion\n",
    "    return (w, -x, -y, -z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function multiples two quaternions\n",
    "\n",
    "def multiply_quaternions(quaternion1, quaternion2):\n",
    "    w1, x1, y1, z1 = quaternion1\n",
    "    w2, x2, y2, z2 = quaternion2\n",
    "    # rearranged a little for my own clarity\n",
    "    w = (w1 * w2) - (x1 * x2) - (y1 * y2) - (z1 * z2)\n",
    "    x = (w1 * x2) + (x1 * w2) + (y1 * z2) - (z1 * y2)\n",
    "    y = (w1 * y2) - (x1 * z2) + (y1 * w2) + (z1 * x2)\n",
    "    z = (w1 * z2) + (x1 * y2) - (y1 * x2) + (z1 * w2)  \n",
    "    return w, x, y, z\n",
    "\n",
    "def multiply_quaternionANDvector(q1, v1):\n",
    "    q1 = normalize(q1,tolerance = 0.00001) \n",
    "    q2 = (0.0,) + v1\n",
    "    return multiply_quaternions(multiply_quaternions(q1, q2), get_quaternion_conjugate(q1))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine into a datastructure from all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfdata= {'time':[],'rotX':[],'rotY':[],'rotZ':[],'total_rawacceleration':[],'rest_time':[],'rest_rotX':[],'rest_rotY':[],'rest_rotZ':[],'rest_total_rawacceleration':[],'healthCode':[]}\n",
    "Accelerometer_df = pd.DataFrame(dfdata, columns =['time','rotX','rotY','rotZ','total_rawacceleration','rest_time','rest_rotX','rest_rotY','rest_rotZ','rest_total_rawacceleration','healthCode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load accelerometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mypkls =['0_2500','2500_5000','5000_7500','7500_10000','10000_12500','12500_15000','15000_17500',\n",
    "         '17500_20000','20000_22500','22500_23093']\n",
    "\n",
    "for entry in range(0,2500):\n",
    "    \n",
    "    if entry % 250 == 0:\n",
    "        print entry\n",
    "        \n",
    "    healthCode = walking_df['healthCode'][entry]\n",
    "    recordId = walking_df['recordId'][entry]\n",
    "    medTimepoint = walking_df['medTimepoint'][entry]\n",
    "    walkPath = walking_df['DMoutboundPaths'][entry]\n",
    "    restPath = walking_df['DMrestPaths'][entry]\n",
    "\n",
    "    walkingrecord = [json.loads(line) for line in open(walkPath)]\n",
    "    restrecord = [json.loads(line) for line in open(restPath)]\n",
    "    \n",
    "    walkingrecordlist=walkingrecord[0]\n",
    "    restrecordlist=restrecord[0]\n",
    "    \n",
    "    barlist = np.arange(len(walkingrecordlist))\n",
    "    restbarlist = np.arange(len(restrecordlist))\n",
    "    \n",
    "    time = []\n",
    "    acceleration_vector = []\n",
    "    attitude_quaternion = []\n",
    "    rest_acceleration_vector = []\n",
    "    attitude_quaternion = []\n",
    "    \n",
    "    time = [(walkingrecordlist[bar]['timestamp']) for bar in barlist]\n",
    "    acceleration_vector = [(walkingrecordlist[bar]['userAcceleration']['x'],\n",
    "                            walkingrecordlist[bar]['userAcceleration']['y'],\n",
    "                            walkingrecordlist[bar]['userAcceleration']['z']) for bar in barlist]\n",
    "    attitude_quaternion = [(walkingrecordlist[bar]['attitude']['w'],\n",
    "                            walkingrecordlist[bar]['attitude']['x'],\n",
    "                            walkingrecordlist[bar]['attitude']['y'],\n",
    "                            walkingrecordlist[bar]['attitude']['z']) for bar in barlist]\n",
    "    \n",
    "    rest_time = [(restrecordlist[restbar]['timestamp']) for restbar in restbarlist]\n",
    "    rest_acceleration_vector = [(restrecordlist[restbar]['userAcceleration']['x'],\n",
    "                            restrecordlist[restbar]['userAcceleration']['y'],\n",
    "                            restrecordlist[restbar]['userAcceleration']['z']) for bar in restbarlist]\n",
    "    rest_attitude_quaternion = [(restrecordlist[restbar]['attitude']['w'],\n",
    "                            restrecordlist[restbar]['attitude']['x'],\n",
    "                            restrecordlist[restbar]['attitude']['y'],\n",
    "                            restrecordlist[restbar]['attitude']['z']) for restbar in restbarlist]\n",
    "    \n",
    "    # Calculate quadratic mean of original acceleration signal - this will be used as another directionless \"axis\" in my feature calculations\n",
    "    [X, Y, Z] = zip(*acceleration_vector) \n",
    "    total_rawacceleration = np.sqrt(np.square(X)+np.square(Y)+np.square(Z))\n",
    "    [restX, restY, restZ] = zip(*rest_acceleration_vector) \n",
    "    rest_total_rawacceleration = np.sqrt(np.square(restX)+np.square(restY)+np.square(restZ))\n",
    "    \n",
    "    # Apply quaternion rotations here\n",
    "    plist = np.arange(len(acceleration_vector))\n",
    "    quatrot = [multiply_quaternionANDvector(attitude_quaternion[p],acceleration_vector[p]) for p in plist]\n",
    "    [rotX, rotY, rotZ] = zip(*quatrot) \n",
    "    \n",
    "    rlist = np.arange(len(rest_acceleration_vector))\n",
    "    rest_quatrot = [multiply_quaternionANDvector(rest_attitude_quaternion[r],rest_acceleration_vector[r]) for r in rlist]\n",
    "    [rest_rotX, rest_rotY, rest_rotZ] = zip(*rest_quatrot) \n",
    "    \n",
    "    dfdata= {'time':[],'rotX':[],'rotY':[],'rotZ':[],'total_rawacceleration':[],'rest_time':[],'rest_rotX':[],'rest_rotY':[],'rest_rotZ':[],'rest_total_rawacceleration':[],'healthCode':[]}\n",
    "\n",
    "    Accelerometer_df = Accelerometer_df.append({'time':time,'rotX':rotX,'rotY':rotY,'rotZ':rotZ,\n",
    "                                                'total_rawacceleration':total_rawacceleration,'rest_time':rest_time,\n",
    "                                                'rest_rotX':rest_rotX,'rest_rotY':rest_rotY,'rest_rotZ':rest_rotZ,\n",
    "                                                'rest_total_rawacceleration':rest_total_rawacceleration,\n",
    "                                                'healthCode':healthCode, 'recordId':recordId,'medTimepoint':medTimepoint},ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross-reference accelerometer data with demographics data and pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add relevant demographics data to walking activity feature data frame\n",
    "Demographics_df = demographics_df.drop(['recordId','createdOn','appVersion','phoneInfo','are-caretaker', 'deep-brain-stimulation',\n",
    "                                        'education','employment','health-history','healthcare-provider',\n",
    "                                        'home-usage','last-smoked','maritalStatus','medical-usage',\n",
    "                                        'medical-usage-yesterday','packs-per-day',\n",
    "                                        'past-participation','phone-usage','race','smartphone',\n",
    "                                        'smoked','surgery','video-usage','years-smoking',\n",
    "                                        'diagnosis-year','medication-start-year','onset-year'],axis=1)\n",
    "Combined_df = pd.merge(Accelerometer_df, Demographics_df, on='healthCode')\n",
    "\n",
    "Final_df.to_pickle('Accelerometer_Demographics_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Plot a sample time series\n",
    "\n",
    "\n",
    "\n",
    "pl.figure()\n",
    "pl.plot(fpr, tpr, 'g',label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "pl.plot([0, 1], [0, 1], 'k--')\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.05])\n",
    "pl.xticks(color = 'k', size = 28)\n",
    "pl.yticks(color = 'k', size = 28)\n",
    "pl.xlabel('False Positive Rate',{'color':'k','fontsize': 28})\n",
    "pl.ylabel('True Positive Rate',{'color':'k','fontsize': 28})\n",
    "pl.title('Receiver operating characteristic',{'color':'k','fontsize': 28})\n",
    "pl.legend(loc=\"lower right\",fontsize= 18)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampling interval and frequency\n",
    "samplingint = 0.01\n",
    "Fs = 1/samplingint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ENTER FILENAME!\n",
    "filename = 'Accelerometer_Demographics_df'\n",
    "\n",
    "thresholdingparam = 1.5\n",
    "peakfindinterval = 80\n",
    "\n",
    "minnumpeaksfound = 5\n",
    "maxnumpeaksfound = 20\n",
    "numpeakintervalstoavg = 5\n",
    "\n",
    "maxpoweratrest = 0.00005\n",
    "\n",
    "FFTsamples = 1024\n",
    "minsignallength = 1200\n",
    "\n",
    "RESTFFTsamples = 512\n",
    "RESTminsignallength = 515\n",
    "\n",
    "# buffer for epochs between steps\n",
    "buff = 5\n",
    "minchunksamples = 90\n",
    "\n",
    "# Accelerometer_df = pd.read_pickle(filename)\n",
    "recordnums = Accelerometer_df.index # important - some indices won't be there since we dropped records before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: Savitzky Golay Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Savitzky Golay filter from http://scipy.github.io/old-wiki/pages/Cookbook/SavitzkyGolay\n",
    "def savitzky_golay(y, window_size, order, deriv=0, rate=1):\n",
    "    try:\n",
    "        window_size = np.abs(np.int(window_size))\n",
    "        order = np.abs(np.int(order))\n",
    "    except ValueError, msg:\n",
    "        raise ValueError(\"window_size and order have to be of type int\")\n",
    "    if window_size % 2 != 1 or window_size < 1:\n",
    "        raise TypeError(\"window_size size must be a positive odd number\")\n",
    "    if window_size < order + 2:\n",
    "        raise TypeError(\"window_size is too small for the polynomials order\")\n",
    "    order_range = range(order+1)\n",
    "    half_window = (window_size -1) // 2\n",
    "    # precompute coefficients\n",
    "    b = np.mat([[k**i for i in order_range] for k in range(-half_window, half_window+1)])\n",
    "    m = np.linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)\n",
    "    # pad the signal at the extremes with values taken from the signal itself\n",
    "    firstvals = y[0] - np.abs( y[1:half_window+1][::-1] - y[0] )\n",
    "    lastvals = y[-1] + np.abs(y[-half_window-1:-1][::-1] - y[-1])\n",
    "    y = np.concatenate((firstvals, y, lastvals))\n",
    "    return np.convolve( m[::-1], y, mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_FFT(timeseries, samplingint):\n",
    "    fouriercoefficients = np.fft.rfft(timeseries)\n",
    "    normalization = 2.0/len(timeseries)\n",
    "    fouriercoefficients = fouriercoefficients*normalization\n",
    "    frequencies = np.fft.rfftfreq(len(timeseries),samplingint)\n",
    "    return frequencies, fouriercoefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: Find Peaks with PeakUtils threshold peak finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_peak_info(timesignal,processedsignal,sampling_int,thresholdingparam,peakfindinterval):\n",
    "    AVGsignal = np.mean(processedsignal)\n",
    "    minthreshold = thresholdingparam*AVGsignal\n",
    "    peakindices = peakutils.indexes(processedsignal ,thres=minthreshold,min_dist=peakfindinterval)\n",
    "    peaktimes = peakindices*sampling_int\n",
    "    peakamplitudes = timesignal[peakindices]\n",
    "    \n",
    "    return peakindices, peaktimes, peakamplitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize dataframe for storing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureinfo = {'healthCode':[],'record':[],'age':[],'gender':[],'professional-diagnosis':[],'recordId':[],\n",
    "               'totpowerX':[],'totpowerY':[],'totpowerZ':[],\n",
    "               'powentropyX':[], 'powentropyY':[],'powentropyZ':[],\n",
    "               'numpeaksX':[],'numpeaksY':[],'numpeaksZ':[],\n",
    "               'MEANpeakintX':[],'MEANpeakintY':[],'MEANpeakintZ':[],\n",
    "               'CVpeakintX':[],'CVpeakintY':[],'CVpeakintZ':[],'duration':[],\n",
    "               'FFT1Z':[],'FFT2Z':[],'FFT3Z':[],\n",
    "               'modeFFT1chunksX':[], 'SDFFT1chunksX':[], 'modeFFT2chunksX':[],'SDFFT2chunksX':[], 'modeFFT3chunksX':[], 'SDFFT3chunksX':[],\n",
    "               'meanpowchunksX':[],'CVpowchunksX':[],'meanentropychunksX':[],'CVentropychunksX':[],\n",
    "               'modeFFT1chunksY':[], 'SDFFT1chunksY':[],'modeFFT2chunksY':[], 'SDFFT2chunksY':[], 'modeFFT3chunksY':[], 'SDFFT3chunksY':[],\n",
    "               'meanpowchunksY':[],'CVpowchunksY':[],'meanentropychunksY':[],'CVentropychunksY':[],\n",
    "               'modeFFT1chunksZ':[], 'SDFFT1chunksZ':[], 'modeFFT2chunksZ':[], 'SDFFT2chunksZ':[], 'modeFFT3chunksZ':[], 'SDFFT3chunksZ':[],\n",
    "               'meanpowchunksZ':[],'CVpowchunksZ':[],'meanentropychunksZ':[],'CVentropychunksZ':[],\n",
    "               'onset_lag':[],'sumabs_acceleration':[]}\n",
    "\n",
    "feature_df = pd.DataFrame(featureinfo,columns =['healthCode','record','age', 'gender','professional-diagnosis','recordId',\n",
    "                                                'totpowerX','totpowerY','totpowerZ',\n",
    "                                                'powentropyX','powentropyY','powentropyZ',\n",
    "                                                'numpeaksX','numpeaksY','numpeaksZ',\n",
    "                                                'MEANpeakintX','MEANpeakintY','MEANpeakintZ',\n",
    "                                                'CVpeakintX','CVpeakintY','CVpeakintZ',\n",
    "                                                'duration','FFT1Z','FFT2Z','FFT3Z',\n",
    "                                                'modeFFT1chunksX','modeFFT2chunksX','SDFFT1chunksX', 'SDFFT2chunksX', 'modeFFT3chunksX', 'SDFFT3chunksX', \n",
    "                                                'meanpowchunksX','CVpowchunksX','meanentropychunksX','CVentropychunksX',\n",
    "                                                'modeFFT1chunksY', 'SDFFT1chunksY', 'modeFFT2chunksY', 'SDFFT2chunksY', 'modeFFT3chunksY', 'SDFFT3chunksY',\n",
    "                                                'meanpowchunksY','CVpowchunksY','meanentropychunksY','CVentropychunksY',\n",
    "                                                'modeFFT1chunksZ', 'SDFFT1chunksZ','modeFFT2chunksZ', 'SDFFT2chunksZ', 'modeFFT3chunksZ', 'SDFFT3chunksZ', \n",
    "                                                'meanpowchunksZ','CVpowchunksZ','meanentropychunksZ','CVentropychunksZ',\n",
    "                                                'onset_lag','sumabs_acceleration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: calculate features for the recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featuregrabber(record,signame,sig,datamat,datamat_YSG,datamat_processed,samplingint,thresholdingparam,peakfindinterval,minsignallength,minnumpeaksfound,buff):\n",
    "    \n",
    "    # initialize lists for storing calculations for each epoch or \"chunk\" between steps\n",
    "    FFT1chunks = []\n",
    "    FFT2chunks = []\n",
    "    FFT3chunks = []\n",
    "    powerchunks = []\n",
    "    entropychunks = []\n",
    "    \n",
    "    # calculate the duration of the recording\n",
    "    duration = (len(datamat[sig]))*samplingint\n",
    "    \n",
    "    # take recordings that are longer than the minimal length\n",
    "    if len(datamat[sig]) >= minsignallength:\n",
    "        [peakindices, peaktimes, peakamplitudes] = get_peak_info(datamat[sig],datamat_processed[sig],samplingint,thresholdingparam,peakfindinterval)\n",
    "        stepintervals = np.diff(peaktimes)\n",
    "        numpeaks = len(peakindices)\n",
    "\n",
    "        # take recordings where the number of peaks discovered is within the desired range\n",
    "        if numpeaks > minnumpeaksfound and numpeaks <= maxnumpeaksfound:\n",
    "            onset_lag = peakindices[0]*samplingint\n",
    "            for k in np.arange(numpeaks-2):\n",
    "                peakstart = peakindices[k]+buff\n",
    "                peakend = peakindices[k+1]-buff\n",
    "                chunklen = peakend-peakstart\n",
    "                midchunk = peakstart+int(chunklen/2)\n",
    "                if chunklen > minchunksamples:\n",
    "                    chunk = datamat_YSG[sig][midchunk-(minchunksamples/2):midchunk+(minchunksamples/2)]\n",
    "                    fYSG_chunk,Pxx_denYSG_chunk = signal.welch(chunk, Fs)#nperseg=128)\n",
    "                    \n",
    "                    freqchunk, ffcoeffchunk = get_FFT(chunk,samplingint)\n",
    "                    ffcoeffchunk = np.absolute(ffcoeffchunk)**2\n",
    "            \n",
    "                    ff_chunkdf = pd.DataFrame({'fouriercoefficients':ffcoeffchunk,'frequencies':freqchunk})\n",
    "                    ff_chunkdf=ff_chunkdf.sort_values('fouriercoefficients',ascending = False)\n",
    "            \n",
    "                    if ff_chunkdf.iloc[0]['frequencies'] != 0:\n",
    "                        FFT1chunks.append(ff_chunkdf.iloc[0]['frequencies'])\n",
    "                        FFT2chunks.append(ff_chunkdf.iloc[1]['frequencies'])\n",
    "                        FFT3chunks.append(ff_chunkdf.iloc[2]['frequencies'])\n",
    "                        totchunkpower = np.trapz(Pxx_denYSG_chunk)\n",
    "\n",
    "                        entropychunk = stats.entropy(Pxx_denYSG_chunk/totchunkpower)\n",
    "                        powerchunks.append(totchunkpower)\n",
    "                        entropychunks.append(entropychunk)\n",
    "                        \n",
    "            \n",
    "            # take a central part of the recording (same length for all recordings), and calculate frequency spectrum features\n",
    "            midsignal = int(len(datamat_YSG[sig])/2)\n",
    "            FFTsignal = datamat_YSG[sig][(midsignal-FFTsamples/2):(midsignal+FFTsamples/2)]\n",
    "            \n",
    "            f,Pow = signal.welch(FFTsignal, Fs,scaling='density')\n",
    "            totpower = np.trapz(Pow)\n",
    "            powentropy = stats.entropy(Pow/totpower)\n",
    "            \n",
    "   \n",
    "            frequencies, fouriercoefficients = get_FFT(FFTsignal,samplingint)\n",
    "            fouriercoefficients = np.absolute(fouriercoefficients)**2\n",
    "            ff_df = pd.DataFrame({'fouriercoefficients':fouriercoefficients,'frequencies':frequencies})\n",
    "            ff_df = ff_df.sort_values('fouriercoefficients',ascending = False)\n",
    "        \n",
    "            if ff_df.iloc[0]['frequencies'] != 0:\n",
    "                FFT1 = ff_df.iloc[0]['frequencies']\n",
    "                FFT2 = ff_df.iloc[1]['frequencies']\n",
    "                FFT3 = ff_df.iloc[2]['frequencies']\n",
    "            \n",
    "            else:\n",
    "                FFT1 = float('nan')\n",
    "                FFT2 = float('nan')\n",
    "                FFT3 = float('nan')\n",
    "        else:\n",
    "            onset_lag = float('nan')\n",
    "            totpower = float('nan')\n",
    "            powentropy = float('nan')\n",
    "            FFT1 = float('nan')\n",
    "            FFT2 = float('nan')\n",
    "            FFT3 = float('nan')\n",
    "     \n",
    "    else:\n",
    "        numpeaks = float('nan')\n",
    "        stepintervals = float('nan')\n",
    "        totpower = float('nan')\n",
    "        powentropy = float('nan')\n",
    "        FFT1 = float('nan')\n",
    "        FFT2 = float('nan')\n",
    "        FFT3 = float('nan')\n",
    "        onset_lag = float('nan')\n",
    "        \n",
    "    return duration, onset_lag, numpeaks, stepintervals, totpower, powentropy, FFT1, FFT2, FFT3, FFT1chunks, FFT2chunks, FFT3chunks, powerchunks, entropychunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: calculate features within epochs or \"chunks\" between steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunkstats(FFT1chunks, FFT2chunks, FFT3chunks, powerchunks, entropychunks):\n",
    "    if len(FFT1chunks)>=1:\n",
    "        modeFFT1chunks = mode(FFT1chunks)[0][0]\n",
    "        meanpowchunks = np.mean(powerchunks)\n",
    "        SDFFT1chunks = np.std(FFT1chunks)\n",
    "        CVpowchunks = np.std(powerchunks) / meanpowchunks\n",
    "        meanentropychunks = np.mean(entropychunks)\n",
    "        CVentropychunks = np.std(entropychunks) / meanentropychunks\n",
    "        \n",
    "    else:\n",
    "        modeFFT1chunks = float('nan')  \n",
    "        meanpowchunks = float('nan')\n",
    "        SDFFT1chunks = float('nan')\n",
    "        CVpowchunks = float('nan')\n",
    "        meanentropychunks = float('nan')\n",
    "        CVentropychunks = float('nan')\n",
    "        \n",
    "    if len(FFT2chunks)>=1:\n",
    "        modeFFT2chunks = mode(FFT2chunks)[0][0]\n",
    "        SDFFT2chunks = np.std(FFT2chunks)\n",
    "        \n",
    "    else:\n",
    "        modeFFT2chunks = float('nan')  \n",
    "        SDFFT2chunks = float('nan')\n",
    "        \n",
    "            \n",
    "    if len(FFT3chunks)>=1:\n",
    "        modeFFT3chunks = mode(FFT3chunks)[0][0]\n",
    "        SDFFT3chunks = np.std(FFT3chunks)\n",
    "        \n",
    "    else:\n",
    "        modeFFT3chunks = float('nan')  \n",
    "        SDFFT3chunks = float('nan')\n",
    "        \n",
    "    return modeFFT1chunks, SDFFT1chunks, modeFFT2chunks, SDFFT2chunks, modeFFT3chunks, SDFFT3chunks, meanpowchunks,CVpowchunks, meanentropychunks,CVentropychunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Go through each record in the data frame, calculate features and store in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for record in recordnums:\n",
    "    \n",
    "    ##################################### WALKING DATA ###################################\n",
    "    \n",
    "    time = Accelerometer_df['time'][record]\n",
    "    datamat = np.array([np.asarray(Accelerometer_df['rotX'][record]), np.asarray(Accelerometer_df['rotY'][record]),\n",
    "                        np.asarray(Accelerometer_df['rotZ'][record]), np.asarray(Accelerometer_df['total_rawacceleration'][record])])\n",
    "\n",
    "    axisitems  = np.arange(0,len(datamat))\n",
    "    \n",
    "    # Apply Savitsky Golay filter\n",
    "    datamat_YSG = [savitzky_golay(datamat[myaxis], window_size =51, order=4) for myaxis in axisitems]\n",
    "    \n",
    "    # Apply Savitsky Golay filter to squared signal for peak finding\n",
    "    datamat_YSG_peaks = [savitzky_golay(np.square(datamat[myaxis]), window_size=51, order=4) for myaxis in axisitems]\n",
    "    datamat_processed = datamat_YSG_peaks\n",
    "     \n",
    "    \n",
    "    # X\n",
    "    durationX, onset_lagX, numpeaksX, stepintervalsX, totpowerX, powentropyX, FFT1X, FFT2X, FFT3X, FFT1chunksX, FFT2chunksX, FFT3chunksX, powerchunksX, entropychunksX = featuregrabber(str(record),'X',0,datamat,datamat_YSG,datamat_processed,samplingint,thresholdingparam,peakfindinterval,minsignallength,minnumpeaksfound,buff)\n",
    "    modeFFT1chunksX, SDFFT1chunksX, modeFFT2chunksX, SDFFT2chunksX, modeFFT3chunksX, SDFFT3chunksX, meanpowchunksX,CVpowchunksX,meanentropychunksX,CVentropychunksX  = chunkstats(FFT1chunksX, FFT2chunksX, FFT3chunksX, powerchunksX, entropychunksX)\n",
    "    \n",
    "    \n",
    "    # Y\n",
    "    durationY, onset_lagY, numpeaksY, stepintervalsY, totpowerY, powentropyY, FFT1Y, FFT2Y, FFT3Y, FFT1chunksY, FFT2chunksY, FFT3chunksY, powerchunksY, entropychunksY = featuregrabber(str(record),'Y',1,datamat,datamat_YSG,datamat_processed,samplingint,thresholdingparam,peakfindinterval,minsignallength,minnumpeaksfound,buff)\n",
    "    modeFFT1chunksY, SDFFT1chunksY, modeFFT2chunksY, SDFFT2chunksY, modeFFT3chunksY, SDFFT3chunksY, meanpowchunksY,CVpowchunksY,meanentropychunksY,CVentropychunksY  = chunkstats(FFT1chunksY, FFT2chunksY, FFT3chunksY, powerchunksY, entropychunksY)\n",
    "    \n",
    "\n",
    "    # Z\n",
    "    durationZ, onset_lagZ, numpeaksZ, stepintervalsZ, totpowerZ, powentropyZ, FFT1Z, FFT2Z, FFT3Z, FFT1chunksZ, FFT2chunksZ, FFT3chunksZ, powerchunksZ, entropychunksZ = featuregrabber(str(record),'Z',2,datamat,datamat_YSG,datamat_processed,samplingint,thresholdingparam,peakfindinterval,minsignallength,minnumpeaksfound,buff)\n",
    "    modeFFT1chunksZ, SDFFT1chunksZ, modeFFT2chunksZ, SDFFT2chunksZ, modeFFT3chunksZ, SDFFT3chunksZ, meanpowchunksZ,CVpowchunksZ,meanentropychunksZ,CVentropychunksZ  = chunkstats(FFT1chunksZ, FFT2chunksZ, FFT3chunksZ, powerchunksZ, entropychunksZ)\n",
    "    \n",
    "    \n",
    "    # QUADRATIC SUM OF X,Y,Z\n",
    "    durationSUM, onset_lagSUM,numpeaksSUM, stepintervalsSUM, totpowerSUM, powentropySUM, FFT1SUM, FFT2SUM, FFT3SUM, FFT1chunksSUM, FFT2chunksSUM, FFT3chunksSUM, powerchunksSUM, entropychunksSUM = featuregrabber(str(record),'SUM',3,datamat,datamat_YSG,datamat_processed,samplingint,thresholdingparam,peakfindinterval,minsignallength,minnumpeaksfound,buff)\n",
    "    modeFFT1chunksSUM, SDFFT1chunksSUM, modeFFT2chunksSUM, SDFFT2chunksSUM, modeFFT3chunksSUM, SDFFT3chunksSUM, meanpowchunksSUM,CVpowchunksSUM,meanentropychunksSUM,CVentropychunksSUM = chunkstats(FFT1chunksSUM, FFT2chunksSUM, FFT3chunksSUM, powerchunksSUM, entropychunksSUM)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get metadata for the record\n",
    "    healthCode = Accelerometer_df['healthCode'][record]\n",
    "    age = Accelerometer_df['age'][record]\n",
    "    gender = Accelerometer_df['gender'][record]\n",
    "    professional_diagnosis = Accelerometer_df['professional-diagnosis'][record]\n",
    "    recordId = Accelerometer_df['recordId'][record]\n",
    "    \n",
    "    # Make final feature calculations\n",
    "    MEANpeakintX = np.mean(stepintervalsX)\n",
    "    MEANpeakintY = np.mean(stepintervalsY)\n",
    "    MEANpeakintZ = np.mean(stepintervalsZ)\n",
    "    CVpeakintX = np.std(stepintervalsX) / MEANpeakintX\n",
    "    CVpeakintY = np.std(stepintervalsY) / MEANpeakintY\n",
    "    CVpeakintZ = np.std(stepintervalsZ) / MEANpeakintZ\n",
    "    duration = durationZ\n",
    "    onset_lag = onset_lagZ\n",
    "    sumabs_acceleration = sum(abs(datamat[3]))\n",
    "\n",
    "   \n",
    "    feature_df = feature_df.append({'healthCode':healthCode,'record':record,'age':age, 'gender':gender, 'professional-diagnosis':professional_diagnosis,'recordId':recordId, \n",
    "                                    'totpowerX':totpowerX,'totpowerY':totpowerY,'totpowerZ':totpowerZ,\n",
    "                                    'powentropyX':powentropyX, 'powentropyY':powentropyY,'powentropyZ':powentropyZ,\n",
    "                                    'numpeaksX':numpeaksX,'numpeaksY':numpeaksY,'numpeaksZ':numpeaksZ,\n",
    "                                    'MEANpeakintX':MEANpeakintX,'MEANpeakintY':MEANpeakintY,'MEANpeakintZ':MEANpeakintZ,\n",
    "                                    'CVpeakintX':CVpeakintX,'CVpeakintY':CVpeakintY,'CVpeakintZ':CVpeakintZ,\n",
    "                                    'duration':duration,'FFT1Z':FFT1Z,'FFT2Z':FFT2Z,'FFT3Z':FFT3Z,  \n",
    "                                    'modeFFT1chunksX':modeFFT1chunksX,'SDFFT1chunksX':SDFFT1chunksX,\n",
    "                                    'modeFFT2chunksX':modeFFT2chunksX,'SDFFT2chunksX':SDFFT2chunksX,\n",
    "                                    'modeFFT3chunksX':modeFFT3chunksX,'SDFFT3chunksX':SDFFT3chunksX,\n",
    "                                    'meanpowchunksX':meanpowchunksX,'CVpowchunksX':CVpowchunksX,\n",
    "                                    'meanentropychunksX':meanentropychunksX,'CVentropychunksX':CVentropychunksX,\n",
    "                                    'modeFFT1chunksY':modeFFT1chunksY,'SDFFT1chunksY':SDFFT1chunksY,\n",
    "                                    'modeFFT2chunksY':modeFFT2chunksY,'SDFFT2chunksY':SDFFT2chunksY,\n",
    "                                    'modeFFT3chunksY':modeFFT3chunksY,'SDFFT3chunksY':SDFFT3chunksY,\n",
    "                                    'meanpowchunksY':meanpowchunksY,'CVpowchunksY':CVpowchunksY,\n",
    "                                    'meanentropychunksY':meanentropychunksY,'CVentropychunksY':CVentropychunksY,\n",
    "                                    'modeFFT1chunksZ':modeFFT1chunksZ,'SDFFT1chunksZ':SDFFT1chunksZ,\n",
    "                                    'modeFFT2chunksZ':modeFFT2chunksZ,'SDFFT2chunksZ':SDFFT2chunksZ,\n",
    "                                    'modeFFT3chunksZ':modeFFT3chunksZ,'SDFFT3chunksZ':SDFFT3chunksZ,\n",
    "                                    'meanpowchunksZ':meanpowchunksZ,'CVpowchunksZ':CVpowchunksZ,\n",
    "                                    'meanentropychunksZ':meanentropychunksZ,'CVentropychunksZ':CVentropychunksZ,\n",
    "                                    'onset_lag':onset_lag,'sumabs_acceleration':sumabs_acceleration},ignore_index=True)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Remove NANs and pickle the resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanfeature_df = feature_dfmod.dropna()\n",
    "cleanfeature_df.to_pickle('feature_df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
